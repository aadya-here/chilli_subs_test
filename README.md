## Lit Mag Submission Calls Aggregator
This project is a full-stack data pipeline and UI designed to collect, normalize, and browse literary magazine submission calls. It automates the collection of data from public sourcesâ€”such as Moksha-powered pages and literary directoriesâ€”storing them in a structured database for easy discovery.

## ğŸ›  Tech Stack
* **Language:** TypeScript (End-to-end for scrapers, API, and UI)
* **Framework:** Next.js (Pages Router)
* **Database:** MongoDB
* **ORM:** Prisma
* **Tooling:** ESLint + Prettier

---

## ğŸ— Architecture

* **Fetcher Layer:** Implements retries, exponential backoff, and polite rate limiting
* **Source Parsers:** Dedicated modules for Moksha-powered sites and public directories (currently it has only Mokhsa)
* **Normalization Engine:** Standardizes casing, canonical URLs, and maps genres to fixed enumms
* **Idempotent Upsert:** Prisma-based logic that prevents duplicates using `normalizedName` and `hostname` constraints

---

### Project Structure

â”œâ”€â”€ prisma/
â”‚ â”œâ”€â”€ schema.prisma 
â”‚ â””â”€â”€ seed.ts 
â”‚
â”œâ”€â”€ src/
â”‚ â”œâ”€â”€ pages
â”‚ â”‚ â”œâ”€â”€ publications/
â”‚ â”‚ â”‚ â”œâ”€â”€ index.ts
â”‚ â”‚ â”‚ â””â”€â”€ [id].ts
â”‚ â”‚ â””â”€â”€ api/publications
â”‚ â”‚ | |â”€â”€ index.tsx
â”‚ â”‚ â”‚ â””â”€â”€ [id].tsx
â”‚ â”‚ â”œâ”€â”€ _app.tsx
â”‚ â”‚ |â”€â”€ index.tsx
â”‚ â”œâ”€â”€ scrape/
â”‚ â”‚ |
â”‚ â”‚ â”œâ”€â”€ miniscraper.ts # test file, can ignore
â”‚ â”‚ â”œâ”€â”€ normalize/
â”‚ â”‚ â”œâ”€â”€ sources/ 
â”‚ â”‚ â”œâ”€â”€ upsert/
â”‚ â”‚
â”‚ â”‚â”€â”€ server
â”‚ â”‚â”€â”€ styles 
â”‚ |â”€â”€ utils
â”œâ”€â”€ .env.example
â”œâ”€â”€ package.json
â”œâ”€â”€ tsconfig.json
â””â”€â”€ README.md



## ğŸš€ Getting Started

### 1. Setup
1. run `pnpm install`

2. create a .env file and add your mongodb url DATABASE_URL="mongodb+srv://..."
3. run `pnpm prisma db generate`  
4.  run `pnpm add -D ts-node` & then `pnpm tsc src/scrape/sources/moksha.ts` to insert to db
5. run `pnpm dev` to see the output 

ğŸ§© Extensibility

Adding a new source requires:
* New parser in src/scrape/sources/
* Mapping to normalized fields
<!-- Registering the source in the CLI -->


